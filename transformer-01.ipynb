{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNkB4+xZm2zN9CuKqUeLUW3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/KOSA-LLM-0629/blob/main/transformer-01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxfK5HKC-F2W"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# download dataset provided by Anki: https://www.manythings.org/anki/\n",
        "text_file = tf.keras.utils.get_file(\n",
        "    fname=\"fra-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "# show where the file is located now\n",
        "text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n",
        "print(text_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# download dataset provided by Anki: https://www.manythings.org/anki/\n",
        "text_file = tf.keras.utils.get_file(\n",
        "    fname=\"fra-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n",
        "\n",
        "def normalize(line):\n",
        "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
        "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
        "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\" \\1\", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\" \\1\", line)\n",
        "    eng, fra = line.split(\"\\t\")\n",
        "    fra = \"[start] \" + fra + \" [end]\"\n",
        "    return eng, fra\n",
        "\n",
        "# normalize each line and separate into English and French\n",
        "with open(text_file) as fp:\n",
        "    text_pairs = [normalize(line) for line in fp]\n",
        "\n",
        "# print some samples\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"wb\") as fp:\n",
        "    pickle.dump(text_pairs, fp)"
      ],
      "metadata": {
        "id": "KI_GDG3k_wUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "    text_pairs = pickle.load(fp)\n",
        "\n",
        "# count tokens\n",
        "eng_tokens, fra_tokens = set(), set()\n",
        "eng_maxlen, fra_maxlen = 0, 0\n",
        "for eng, fra in text_pairs:\n",
        "    eng_tok, fra_tok = eng.split(), fra.split()\n",
        "    eng_maxlen = max(eng_maxlen, len(eng_tok))\n",
        "    fra_maxlen = max(fra_maxlen, len(fra_tok))\n",
        "    eng_tokens.update(eng_tok)\n",
        "    fra_tokens.update(fra_tok)\n",
        "print(f\"Total English tokens: {len(eng_tokens)}\")\n",
        "print(f\"Total French tokens: {len(fra_tokens)}\")\n",
        "print(f\"Max English length: {eng_maxlen}\")\n",
        "print(f\"Max French length: {fra_maxlen}\")\n",
        "print(f\"{len(text_pairs)} total pairs\")"
      ],
      "metadata": {
        "id": "gfLqxWXjAOOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "    text_pairs = pickle.load(fp)\n",
        "\n",
        "# histogram of sentence length in tokens\n",
        "en_lengths = [len(eng.split()) for eng, fra in text_pairs]\n",
        "fr_lengths = [len(fra.split()) for eng, fra in text_pairs]\n",
        "\n",
        "plt.hist(en_lengths, label=\"en\", color=\"red\", alpha=0.33)\n",
        "plt.hist(fr_lengths, label=\"fr\", color=\"blue\", alpha=0.33)\n",
        "plt.yscale(\"log\")     # sentence length fits Benford\"s law\n",
        "plt.ylim(plt.ylim())  # make y-axis consistent for both plots\n",
        "plt.plot([max(en_lengths), max(en_lengths)], plt.ylim(), color=\"red\")\n",
        "plt.plot([max(fr_lengths), max(fr_lengths)], plt.ylim(), color=\"blue\")\n",
        "plt.legend()\n",
        "plt.title(\"Examples count vs Token length\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OFJKx3Z3Abdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Load normalized sentence pairs\n",
        "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "    text_pairs = pickle.load(fp)\n",
        "\n",
        "# train-test-val split of randomized sentence pairs\n",
        "random.shuffle(text_pairs)\n",
        "n_val = int(0.15*len(text_pairs))\n",
        "n_train = len(text_pairs) - 2*n_val\n",
        "train_pairs = text_pairs[:n_train]\n",
        "val_pairs = text_pairs[n_train:n_train+n_val]\n",
        "test_pairs = text_pairs[n_train+n_val:]\n",
        "\n",
        "# Parameter determined after analyzing the input data\n",
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "seq_length = 20\n",
        "\n",
        "# Create vectorizer\n",
        "eng_vectorizer = TextVectorization(\n",
        "    max_tokens=vocab_size_en,\n",
        "    standardize=None,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=seq_length,\n",
        ")\n",
        "fra_vectorizer = TextVectorization(\n",
        "    max_tokens=vocab_size_fr,\n",
        "    standardize=None,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=seq_length + 1\n",
        ")\n",
        "\n",
        "# train the vectorization layer using training dataset\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_fra_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorizer.adapt(train_eng_texts)\n",
        "fra_vectorizer.adapt(train_fra_texts)\n",
        "\n",
        "# save for subsequent steps\n",
        "with open(\"vectorize.pickle\", \"wb\") as fp:\n",
        "    data = {\n",
        "        \"train\": train_pairs,\n",
        "        \"val\":   val_pairs,\n",
        "        \"test\":  test_pairs,\n",
        "        \"engvec_config\":  eng_vectorizer.get_config(),\n",
        "        \"engvec_weights\": eng_vectorizer.get_weights(),\n",
        "        \"fravec_config\":  fra_vectorizer.get_config(),\n",
        "        \"fravec_weights\": fra_vectorizer.get_weights(),\n",
        "    }\n",
        "    pickle.dump(data, fp)"
      ],
      "metadata": {
        "id": "eHHF3vfuAqOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# load text data and vectorizer weights\n",
        "with open(\"vectorize.pickle\", \"rb\") as fp:\n",
        "    data = pickle.load(fp)\n",
        "\n",
        "train_pairs = data[\"train\"]\n",
        "val_pairs = data[\"val\"]\n",
        "test_pairs = data[\"test\"]   # not used\n",
        "\n",
        "eng_vectorizer = TextVectorization.from_config(data[\"engvec_config\"])\n",
        "eng_vectorizer.set_weights(data[\"engvec_weights\"])\n",
        "fra_vectorizer = TextVectorization.from_config(data[\"fravec_config\"])\n",
        "fra_vectorizer.set_weights(data[\"fravec_weights\"])\n",
        "\n",
        "# set up Dataset object\n",
        "def format_dataset(eng, fra):\n",
        "    \"\"\"Take an English and a French sentence pair, convert into input and target.\n",
        "    The input is a dict with keys `encoder_inputs` and `decoder_inputs`, each\n",
        "    is a vector, corresponding to English and French sentences respectively.\n",
        "    The target is also vector of the French sentence, advanced by 1 token. All\n",
        "    vector are in the same length.\n",
        "\n",
        "    The output will be used for training the transformer model. In the model we\n",
        "    will create, the input tensors are named `encoder_inputs` and `decoder_inputs`\n",
        "    which should be matched to the keys in the dictionary for the source part\n",
        "    \"\"\"\n",
        "    eng = eng_vectorizer(eng)\n",
        "    fra = fra_vectorizer(fra)\n",
        "    source = {\"encoder_inputs\": eng,\n",
        "              \"decoder_inputs\": fra[:, :-1]}\n",
        "    target = fra[:, 1:]\n",
        "    return (source, target)\n",
        "\n",
        "def make_dataset(pairs, batch_size=64):\n",
        "    \"\"\"Create TensorFlow Dataset for the sentence pairs\"\"\"\n",
        "    # aggregate sentences using zip(*pairs)\n",
        "    eng_texts, fra_texts = zip(*pairs)\n",
        "    # convert them into list, and then create tensors\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(fra_texts)))\n",
        "    return dataset.shuffle(2048) \\\n",
        "                  .batch(batch_size).map(format_dataset) \\\n",
        "                  .prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "# test the dataset\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"encoder_inputs\"][0]: {inputs[\"encoder_inputs\"][0]}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"][0]: {inputs[\"decoder_inputs\"][0]}')\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "    print(f\"targets[0]: {targets[0]}\")"
      ],
      "metadata": {
        "id": "rbR3h6MgBFRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def pos_enc_matrix(L, d, n=10000):\n",
        "    \"\"\"Create positional encoding matrix\n",
        "\n",
        "    Args:\n",
        "        L: Input dimension (length)\n",
        "        d: Output dimension (depth), even only\n",
        "        n: Constant for the sinusoidal functions\n",
        "\n",
        "    Returns:\n",
        "        numpy matrix of floats of dimension L-by-d. At element (k,2i) the value\n",
        "        is sin(k/n^(2i/d)) while at element (k,2i+1) the value is cos(k/n^(2i/d))\n",
        "    \"\"\"\n",
        "    assert d % 2 == 0, \"Output dimension needs to be an even integer\"\n",
        "    d2 = d//2\n",
        "    P = np.zeros((L, d))\n",
        "    k = np.arange(L).reshape(-1, 1)     # L-column vector\n",
        "    i = np.arange(d2).reshape(1, -1)    # d-row vector\n",
        "    denom = np.power(n, -i/d2)          # n**(-2*i/d)\n",
        "    args = k * denom                    # (L,d) matrix\n",
        "    P[:, ::2] = np.sin(args)\n",
        "    P[:, 1::2] = np.cos(args)\n",
        "    return P\n",
        "\n",
        "\n",
        "# Plot the positional encoding matrix\n",
        "pos_matrix = pos_enc_matrix(L=2048, d=512)\n",
        "assert pos_matrix.shape == (2048, 512)\n",
        "plt.pcolormesh(pos_matrix, cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "with open(\"posenc-2048-512.pickle\", \"wb\") as fp:\n",
        "    pickle.dump(pos_matrix, fp)"
      ],
      "metadata": {
        "id": "jGFO5P68B0q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "with open(\"posenc-2048-512.pickle\", \"rb\") as fp:\n",
        "    pos_matrix = pickle.load(fp)\n",
        "assert pos_matrix.shape == (2048, 512)\n",
        "# Plot the positional encoding matrix, alternative way\n",
        "plt.pcolormesh(np.hstack([pos_matrix[:, ::2], pos_matrix[:, 1::2]]), cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kJfWyREbCETM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "plt.plot(pos_matrix[:, 155], label=\"high freq\")\n",
        "plt.plot(pos_matrix[:, 300], label=\"low freq\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zuKyZtb1CJSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with open(\"posenc-2048-512.pickle\", \"rb\") as fp:\n",
        "    pos_matrix = pickle.load(fp)\n",
        "assert pos_matrix.shape == (2048, 512)\n",
        "# Plot two curves from different position\n",
        "plt.plot(pos_matrix[100], alpha=0.66, color=\"red\", label=\"position 100\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mqepZfUpCMoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "with open(\"posenc-2048-512.pickle\", \"rb\") as fp:\n",
        "    pos_matrix = pickle.load(fp)\n",
        "assert pos_matrix.shape == (2048, 512)\n",
        "# Show the dot product between different normalized positional vectors\n",
        "pos_matrix /= np.linalg.norm(pos_matrix, axis=1, keepdims=True)\n",
        "p = pos_matrix[789]  # all vectors compare to vector at position 789\n",
        "dots = pos_matrix @ p\n",
        "plt.plot(dots)\n",
        "plt.ylim([0, 1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "beAs_ipiCYPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def pos_enc_matrix(L, d, n=10000):\n",
        "    \"\"\"Create positional encoding matrix\n",
        "\n",
        "    Args:\n",
        "        L: Input dimension (length)\n",
        "        d: Output dimension (depth), even only\n",
        "        n: Constant for the sinusoidal functions\n",
        "\n",
        "    Returns:\n",
        "        numpy matrix of floats of dimension L-by-d. At element (k,2i) the value\n",
        "        is sin(k/n^(2i/d)) while at element (k,2i+1) the value is cos(k/n^(2i/d))\n",
        "    \"\"\"\n",
        "    assert d % 2 == 0, \"Output dimension needs to be an even integer\"\n",
        "    d2 = d//2\n",
        "    P = np.zeros((L, d))\n",
        "    k = np.arange(L).reshape(-1, 1)     # L-column vector\n",
        "    i = np.arange(d2).reshape(1, -1)    # d-row vector\n",
        "    denom = np.power(n, -i/d2)          # n**(-2*i/d)\n",
        "    args = k * denom                    # (L,d) matrix\n",
        "    P[:, ::2] = np.sin(args)\n",
        "    P[:, 1::2] = np.cos(args)\n",
        "    return P\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    \"\"\"Positional embedding layer. Assume tokenized input, transform into\n",
        "    embedding and returns positional-encoded output.\"\"\"\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequence_length: Input sequence length\n",
        "            vocab_size: Input vocab size, for setting up embedding matrix\n",
        "            embed_dim: Embedding vector size, for setting up embedding matrix\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim     # d_model in paper\n",
        "        # token embedding layer: Convert integer token to D-dim float vector\n",
        "        self.token_embeddings = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim, mask_zero=True\n",
        "        )\n",
        "        # positional embedding layer: a matrix of hard-coded sine values\n",
        "        matrix = pos_enc_matrix(sequence_length, embed_dim)\n",
        "        self.position_embeddings = tf.constant(matrix, dtype=\"float32\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Input tokens convert into embedding vectors then superimposed\n",
        "        with position vectors\"\"\"\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        return embedded_tokens + self.position_embeddings\n",
        "\n",
        "    # this layer is using an Embedding layer, which can take a mask\n",
        "    # see https://www.tensorflow.org/guide/keras/masking_and_padding#passing_mask_tensors_directly_to_layers\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.token_embeddings.compute_mask(*args, **kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        # to make save and load a model using custom layer possible\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "9Nca-d7oClwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From Lesson 03:\n",
        "# train_ds = make_dataset(train_pairs)\n",
        "\n",
        "vocab_size_en = 10000\n",
        "seq_length = 20\n",
        "\n",
        "# test the dataset\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(inputs[\"encoder_inputs\"])\n",
        "    embed_en = PositionalEmbedding(seq_length, vocab_size_en, embed_dim=512)\n",
        "    en_emb = embed_en(inputs[\"encoder_inputs\"])\n",
        "    print(en_emb.shape)\n",
        "    print(en_emb._keras_mask)"
      ],
      "metadata": {
        "id": "5lPZwlNUDDNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def self_attention(input_shape, prefix=\"att\", mask=False, **kwargs):\n",
        "    \"\"\"Self-attention layers at transformer encoder and decoder. Assumes its\n",
        "    input is the output from positional encoding layer.\n",
        "\n",
        "    Args:\n",
        "        prefix (str): The prefix added to the layer names\n",
        "        masked (bool): whether to use causal mask. Should be False on encoder and\n",
        "                       True on decoder. When True, a mask will be applied such that\n",
        "                       each location only has access to the locations before it.\n",
        "    \"\"\"\n",
        "    # create layers\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in1\")\n",
        "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn1\", **kwargs)\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm1\")\n",
        "    add = tf.keras.layers.Add(name=f\"{prefix}_add1\")\n",
        "    # functional API to connect input to output\n",
        "    attout = attention(query=inputs, value=inputs, key=inputs,\n",
        "                       use_causal_mask=mask)\n",
        "    outputs = norm(add([inputs, attout]))\n",
        "    # create model and return\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_att\")\n",
        "    return model\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "num_heads = 8\n",
        "\n",
        "model = self_attention(input_shape=(seq_length, key_dim),\n",
        "                       num_heads=num_heads, key_dim=key_dim)\n",
        "tf.keras.utils.plot_model(model, \"self-attention.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "THhlrTgwDNo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the decoder, you have a cross-attention model that takes input from the self-attention model as well as the encoder. In this case, the value and key are the output from the encoder whereas the query is the output from the self-attention model.\n"
      ],
      "metadata": {
        "id": "VYcw_l6QD1L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def cross_attention(input_shape, context_shape, prefix=\"att\", **kwargs):\n",
        "    \"\"\"Cross-attention layers at transformer decoder. Assumes its\n",
        "    input is the output from positional encoding layer at decoder\n",
        "    and context is the final output from encoder.\n",
        "\n",
        "    Args:\n",
        "        prefix (str): The prefix added to the layer names\n",
        "    \"\"\"\n",
        "    # create layers\n",
        "    context = tf.keras.layers.Input(shape=context_shape, dtype='float32',\n",
        "                                    name=f\"{prefix}_ctx2\")\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in2\")\n",
        "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn2\", **kwargs)\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm2\")\n",
        "    add = tf.keras.layers.Add(name=f\"{prefix}_add2\")\n",
        "    # functional API to connect input to output\n",
        "    attout = attention(query=inputs, value=context, key=context)\n",
        "    outputs = norm(add([attout, inputs]))\n",
        "    # create model and return\n",
        "    model = tf.keras.Model(inputs=[(context, inputs)], outputs=outputs,\n",
        "                           name=f\"{prefix}_cross\")\n",
        "    return model\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "num_heads = 8\n",
        "\n",
        "model = cross_attention(input_shape=(seq_length, key_dim),\n",
        "                        context_shape=(seq_length, key_dim),\n",
        "                        num_heads=num_heads, key_dim=key_dim)\n",
        "tf.keras.utils.plot_model(model, \"cross-attention.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "j5uuAKcQD2gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def feed_forward(input_shape, model_dim, ff_dim, dropout=0.1, prefix=\"ff\"):\n",
        "    \"\"\"Feed-forward layers at transformer encoder and decoder. Assumes its\n",
        "    input is the output from an attention layer with add & norm, the output\n",
        "    is the output of one encoder or decoder block\n",
        "\n",
        "    Args:\n",
        "        model_dim (int): Output dimension of the feed-forward layer, which\n",
        "                         is also the output dimension of the encoder/decoder\n",
        "                         block\n",
        "        ff_dim (int): Internal dimension of the feed-forward layer\n",
        "        dropout (float): Dropout rate\n",
        "        prefix (str): The prefix added to the layer names\n",
        "    \"\"\"\n",
        "    # create layers\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in3\")\n",
        "    dense1 = tf.keras.layers.Dense(ff_dim, name=f\"{prefix}_ff1\", activation=\"relu\")\n",
        "    dense2 = tf.keras.layers.Dense(model_dim, name=f\"{prefix}_ff2\")\n",
        "    drop = tf.keras.layers.Dropout(dropout, name=f\"{prefix}_drop\")\n",
        "    add = tf.keras.layers.Add(name=f\"{prefix}_add3\")\n",
        "    # functional API to connect input to output\n",
        "    ffout = drop(dense2(dense1(inputs)))\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm3\")\n",
        "    outputs = norm(add([inputs, ffout]))\n",
        "    # create model and return\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_ff\")\n",
        "    return model\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "\n",
        "model = feed_forward(input_shape=(seq_length, key_dim),\n",
        "                     model_dim=key_dim, ff_dim=ff_dim)\n",
        "tf.keras.utils.plot_model(model, \"feedforward.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "CGgap4bDEFL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the encoder is the self-attention submodel connected to the feed-forward submodel. The decoder, on the other hand, is a self-attention submodel, a cross-attention submodel, and a feed-forward submodel connected in tandem."
      ],
      "metadata": {
        "id": "9D5l62IMEW3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# the building block functions from Lesson 06\n",
        "#from lesson_06 import self_attention, feed_forward\n",
        "\n",
        "def encoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"enc\", **kwargs):\n",
        "    \"\"\"One encoder unit. The input and output are in the same shape so we can\n",
        "    daisy chain multiple encoder units into one larger encoder\"\"\"\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f\"{prefix}_in0\"),\n",
        "        self_attention(input_shape, prefix=prefix, key_dim=key_dim, mask=False, **kwargs),\n",
        "        feed_forward(input_shape, key_dim, ff_dim, dropout, prefix),\n",
        "    ], name=prefix)\n",
        "    return model\n",
        "\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "num_heads = 8\n",
        "\n",
        "model = encoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim,\n",
        "                num_heads=num_heads)\n",
        "tf.keras.utils.plot_model(model, \"encoder.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "geummhXzEX8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# the three building block functions from Lesson 06\n",
        "#from lesson_06 import self_attention, cross_attention, feed_forward\n",
        "\n",
        "def decoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"dec\", **kwargs):\n",
        "    \"\"\"One decoder unit. The input and output are in the same shape so we can\n",
        "    daisy chain multiple decoder units into one larger decoder. The context\n",
        "    vector is also assumed to be the same shape for convenience\"\"\"\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in0\")\n",
        "    context = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                    name=f\"{prefix}_ctx0\")\n",
        "    attmodel = self_attention(input_shape, key_dim=key_dim, mask=True,\n",
        "                              prefix=prefix, **kwargs)\n",
        "    crossmodel = cross_attention(input_shape, input_shape, key_dim=key_dim,\n",
        "                                 prefix=prefix, **kwargs)\n",
        "    ffmodel = feed_forward(input_shape, key_dim, ff_dim, dropout, prefix)\n",
        "    x = attmodel(inputs)\n",
        "    x = crossmodel([(context, x)])\n",
        "    output = ffmodel(x)\n",
        "    model = tf.keras.Model(inputs=[(inputs, context)], outputs=output, name=prefix)\n",
        "    return model\n",
        "\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "num_heads = 8\n",
        "\n",
        "model = decoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim,\n",
        "                num_heads=num_heads)\n",
        "tf.keras.utils.plot_model(model, \"decoder.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "zMol7k7XEn6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# the positional embedding layer from Lesson 05\n",
        "#from lesson_05 import PositionalEmbedding\n",
        "# the building block functions from Lesson 07\n",
        "#from lesson_07 import encoder, decoder\n",
        "\n",
        "\n",
        "def transformer(num_layers, num_heads, seq_len, key_dim, ff_dim, vocab_size_src,\n",
        "                vocab_size_tgt, dropout=0.1, name=\"transformer\"):\n",
        "    embed_shape = (seq_len, key_dim)  # output shape of the positional embedding layer\n",
        "    # set up layers\n",
        "    input_enc = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n",
        "                                      name=\"encoder_inputs\")\n",
        "    input_dec = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n",
        "                                      name=\"decoder_inputs\")\n",
        "    embed_enc = PositionalEmbedding(seq_len, vocab_size_src, key_dim, name=\"embed_enc\")\n",
        "    embed_dec = PositionalEmbedding(seq_len, vocab_size_tgt, key_dim, name=\"embed_dec\")\n",
        "    encoders = [encoder(input_shape=embed_shape, key_dim=key_dim,\n",
        "                        ff_dim=ff_dim, dropout=dropout, prefix=f\"enc{i}\",\n",
        "                        num_heads=num_heads)\n",
        "                for i in range(num_layers)]\n",
        "    decoders = [decoder(input_shape=embed_shape, key_dim=key_dim,\n",
        "                        ff_dim=ff_dim, dropout=dropout, prefix=f\"dec{i}\",\n",
        "                        num_heads=num_heads)\n",
        "                for i in range(num_layers)]\n",
        "    final = tf.keras.layers.Dense(vocab_size_tgt, name=\"linear\")\n",
        "    # build output\n",
        "    x1 = embed_enc(input_enc)\n",
        "    x2 = embed_dec(input_dec)\n",
        "    for layer in encoders:\n",
        "        x1 = layer(x1)\n",
        "    for layer in decoders:\n",
        "        x2 = layer([x2, x1])\n",
        "    output = final(x2)\n",
        "    # XXX keep this try-except block\n",
        "    try:\n",
        "        del output._keras_mask\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    model = tf.keras.Model(inputs=[input_enc, input_dec], outputs=output, name=name)\n",
        "    return model\n",
        "\n",
        "\n",
        "seq_len = 20\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "dropout = 0.1\n",
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n",
        "                    vocab_size_en, vocab_size_fr, dropout)\n",
        "tf.keras.utils.plot_model(model, \"transformer.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "sBrHC01-E47S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"Custom learning rate for Adam optimizer\"\n",
        "    def __init__(self, key_dim, warmup_steps=4000):\n",
        "        super().__init__()\n",
        "        self.key_dim = key_dim\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.d = tf.cast(self.key_dim, tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        # to make save and load a model using custom layer possible0\n",
        "        config = {\n",
        "            \"key_dim\": self.key_dim,\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "        }\n",
        "        return config\n",
        "\n",
        "key_dim = 128\n",
        "lr = CustomSchedule(key_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "plt.plot(lr(tf.range(50000, dtype=tf.float32)))\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Train Step')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HhJybPQWFInl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(label, pred):\n",
        "    mask = label != 0\n",
        "\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_object(label, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "    pred = tf.argmax(pred, axis=2)\n",
        "    label = tf.cast(label, pred.dtype)\n",
        "    match = label == pred\n",
        "\n",
        "    mask = label != 0\n",
        "\n",
        "    match = match & mask\n",
        "\n",
        "    match = tf.cast(match, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "nK3AEjK8FRS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "seq_len = 20\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "dropout = 0.1\n",
        "model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n",
        "                    vocab_size_en, vocab_size_fr, dropout)\n",
        "lr = CustomSchedule(key_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "model.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "uZmkyhh7FS4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# the dataset objects from Lesson 03\n",
        "#from lesson_03 import train_ds, val_ds\n",
        "# the building block functions from Lesson 08\n",
        "#from lesson_08 import transformer\n",
        "# the learning rate schedule, loss, and accuracy functions from Lesson 09\n",
        "#from lesson_09 import CustomSchedule, masked_loss, masked_accuracy\n",
        "\n",
        "\n",
        "# Create and train the model\n",
        "seq_len = 20\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "dropout = 0.1\n",
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n",
        "                    vocab_size_en, vocab_size_fr, dropout)\n",
        "lr = CustomSchedule(key_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "model.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\n",
        "epochs = 20\n",
        "history = model.fit(train_ds, epochs=epochs, validation_data=val_ds)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"eng-fra-transformer.h5\")\n",
        "\n",
        "# Plot the loss and accuracy history\n",
        "fig, axs = plt.subplots(2, figsize=(6, 8), sharex=True)\n",
        "fig.suptitle('Traininig history')\n",
        "x = list(range(1, epochs+1))\n",
        "axs[0].plot(x, history.history[\"loss\"], alpha=0.5, label=\"loss\")\n",
        "axs[0].plot(x, history.history[\"val_loss\"], alpha=0.5, label=\"val_loss\")\n",
        "axs[0].set_ylabel(\"Loss\")\n",
        "axs[0].legend(loc=\"upper right\")\n",
        "axs[1].plot(x, history.history[\"masked_accuracy\"], alpha=0.5, label=\"acc\")\n",
        "axs[1].plot(x, history.history[\"val_masked_accuracy\"], alpha=0.5, label=\"val_acc\")\n",
        "axs[1].set_ylabel(\"Accuracy\")\n",
        "axs[1].set_xlabel(\"epoch\")\n",
        "axs[1].legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y56FgXUiFYIs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}