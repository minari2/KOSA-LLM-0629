{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlbXxU+gDT05WhdwBJ0EG5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/KOSA-LLM-0629/blob/main/KULLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### https://github.com/nlpai-lab/KULLM"
      ],
      "metadata": {
        "id": "y1DoTXuqTInb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqoDTvNcSvND",
        "outputId": "668db977-dd91-4f7e-db7e-ca1e678ce24e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.13.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch transformers tokenizers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A dedicated helper to manage templates and prompt building.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os.path as osp\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "class Prompter(object):\n",
        "    __slots__ = (\"template\", \"_verbose\")\n",
        "\n",
        "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
        "        self._verbose = verbose\n",
        "        if not template_name:\n",
        "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
        "            template_name = \"alpaca\"\n",
        "        file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
        "        if not osp.exists(file_name):\n",
        "            raise ValueError(f\"Can't read {file_name}\")\n",
        "        with open(file_name) as fp:\n",
        "            self.template = json.load(fp)\n",
        "        if self._verbose:\n",
        "            print(\n",
        "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
        "            )\n",
        "\n",
        "    def generate_prompt(\n",
        "        self,\n",
        "        instruction: str,\n",
        "        input: Union[None, str] = None,\n",
        "        label: Union[None, str] = None,\n",
        "    ) -> str:\n",
        "        # returns the full prompt from instruction and optional input\n",
        "        # if a label (=response, =output) is provided, it's also appended.\n",
        "        if input:\n",
        "            res = self.template[\"prompt_input\"].format(\n",
        "                instruction=instruction, input=input\n",
        "            )\n",
        "        else:\n",
        "            res = self.template[\"prompt_no_input\"].format(\n",
        "                instruction=instruction\n",
        "            )\n",
        "        if label:\n",
        "            res = f\"{res}{label}\"\n",
        "        if self._verbose:\n",
        "            print(res)\n",
        "        return res\n",
        "\n",
        "    def get_response(self, output: str) -> str:\n",
        "        return output.split(self.template[\"response_split\"])[1].strip()"
      ],
      "metadata": {
        "id": "_ygRUM6rVyz8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir utils"
      ],
      "metadata": {
        "id": "NCETdyqiW0AX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A dedicated helper to manage templates and prompt building.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import os.path as osp\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "class Prompter(object):\n",
        "    __slots__ = (\"template\", \"_verbose\")\n",
        "\n",
        "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
        "        self._verbose = verbose\n",
        "        if not template_name:\n",
        "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
        "            template_name = \"alpaca\"\n",
        "        file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
        "        if not osp.exists(file_name):\n",
        "            raise ValueError(f\"Can't read {file_name}\")\n",
        "        with open(file_name) as fp:\n",
        "            self.template = json.load(fp)\n",
        "        if self._verbose:\n",
        "            print(\n",
        "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
        "            )\n",
        "\n",
        "    def generate_prompt(\n",
        "        self,\n",
        "        instruction: str,\n",
        "        input: Union[None, str] = None,\n",
        "        label: Union[None, str] = None,\n",
        "    ) -> str:\n",
        "        # returns the full prompt from instruction and optional input\n",
        "        # if a label (=response, =output) is provided, it's also appended.\n",
        "        if input:\n",
        "            res = self.template[\"prompt_input\"].format(\n",
        "                instruction=instruction, input=input\n",
        "            )\n",
        "        else:\n",
        "            res = self.template[\"prompt_no_input\"].format(\n",
        "                instruction=instruction\n",
        "            )\n",
        "        if label:\n",
        "            res = f\"{res}{label}\"\n",
        "        if self._verbose:\n",
        "            print(res)\n",
        "        return res\n",
        "\n",
        "    def get_response(self, output: str) -> str:\n",
        "        return output.split(self.template[\"response_split\"])[1].strip()"
      ],
      "metadata": {
        "id": "4WHObfU_X3BF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# from utils.prompter import Prompter\n",
        "\n",
        "MODEL = \"nlpai-lab/kullm-polyglot-5.8b-v2\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ").to(device=f\"cuda\", non_blocking=True)\n",
        "model.eval()\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=MODEL, device=0)\n",
        "\n",
        "prompter = Prompter(\"kullm\")\n",
        "\n",
        "\n",
        "def infer(instruction=\"\", input_text=\"\"):\n",
        "    prompt = prompter.generate_prompt(instruction, input_text)\n",
        "    output = pipe(prompt, max_length=512, temperature=0.2, num_beams=5, eos_token_id=2)\n",
        "    s = output[0][\"generated_text\"]\n",
        "    result = prompter.get_response(s)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "result = infer(input_text=\"고려대학교에 대해서 알려줘\")\n",
        "print(result)\n",
        "# '고려대학교에 대해 궁금한 점이 있으시면 언제든지 문의해 주세요. 고려대학교는 한국에서 가장 오래되고 권위 있는 대학교 중 하나로, 고려대학교의 역사는 한국의 역사와 함께해 왔습니다. 고려대학교는 학문적 우수성을 추구하는 동시에 사회적 책임을 다하기 위해 최선을 다하고 있습니다. 고려대학교는 학생, 교수진, 교직원을 위한 다양한 프로그램과 지원을 제공하는 것으로 유명합니다. 고려대학교는 한국의 정치, 경제, 사회 분야에서 중요한 역할을 담당하고 있습니다. 고려대학교에 대해 더 자세히 알고 싶으신가요?'"
      ],
      "metadata": {
        "id": "dnw1iNb0S4Xh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}